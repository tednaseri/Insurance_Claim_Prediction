{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "binding-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "import re\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Quality of image:\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inclusive-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bar_plot(X, y, sort=False, x_label = None, y_label=None, title='Title', x_Rotation = 0, width=5, height=3, annot=False):\n",
    "    fig, ax = plt.subplots(figsize=(width,height))\n",
    "    df = pd.DataFrame({'X_Value':X, 'y_Value':y})\n",
    "    if(sort == True):\n",
    "        df.sort_values(by='y_Value', ascending=False, inplace=True)\n",
    "\n",
    "    sns.barplot(ax=ax, x=df['X_Value'], y=df['y_Value'])\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel(y_label, fontsize=12)\n",
    "    \n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.xticks(rotation=x_Rotation);\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    if(annot == True):\n",
    "        for p in ax.patches:\n",
    "            x=p.get_bbox().get_points()[:,0]\n",
    "            y=p.get_bbox().get_points()[1,1]\n",
    "            ax.annotate('{:.2f}'.format(y), (x.mean(), y), ha='center', va='bottom')    \n",
    "\n",
    "def make_count_plot(df, cols, annotate=False, xRotation = 0, title=None, width=5, height=3):\n",
    "    \"\"\"\n",
    "    This function will calculate and show count plot.\n",
    "    Input: df: a dataframe, \n",
    "           cols: a list of feature names for count plot.\n",
    "           annotate: a condition flag, when it is True the percentage of each bar will be observable. The default value is False. \n",
    "           xRotation: a degree for rotaion of x-axis ticks. The default value is 0.\n",
    "           title: the title of the plot,\n",
    "           width: the width of the plot. The default value is 5.\n",
    "           height: the height of the plot. The default value is 3.\n",
    "    output: This function only shows count plot.\n",
    "    \"\"\"\n",
    "    totalcnt = df.shape[0]\n",
    "    cnt = len(cols)\n",
    "    if(cnt == 1):\n",
    "        fig, ax = plt.subplots(figsize=(width,height*cnt))\n",
    "        col = cols[0]\n",
    "        sns.countplot(x=df[col], ax=ax);\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Frequency');\n",
    "        \n",
    "        if(annotate == True):\n",
    "                for p in ax.patches:\n",
    "                    x=p.get_bbox().get_points()[:,0]\n",
    "                    y=p.get_bbox().get_points()[1,1]\n",
    "                    ax.annotate('{:.1f}%'.format(y*100/totalcnt), (x.mean(), y), ha='center', va='bottom')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(cnt, 1, figsize=(5,3*cnt))\n",
    "        for i, col in enumerate(cols):\n",
    "            sns.countplot(x=df[col], ax=axes[i]);\n",
    "            #axes[i].set_title('Distribution')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Frequency');\n",
    "            if(xRotation != 0):\n",
    "                axes[i].tick_params(axis='x', labelrotation=xRotation)\n",
    "            \n",
    "            if(annotate == True):\n",
    "                for p in axes[i].patches:\n",
    "                    x=p.get_bbox().get_points()[:,0]\n",
    "                    y=p.get_bbox().get_points()[1,1]\n",
    "                    axes[i].annotate('{:.1f}%'.format(y*100/totalcnt), (x.mean(), y), ha='center', va='bottom')\n",
    "\n",
    "        fig.tight_layout()\n",
    "    return\n",
    "\n",
    "\n",
    "def make_box_plot(df, cols):\n",
    "    \"\"\"\n",
    "    This function will show box plot for the passed features.\n",
    "    Input: df: a dataframe, \n",
    "           cols: a list of feature names for box plot.\n",
    "    output: This function only show box plot.\n",
    "    \"\"\"\n",
    "    cnt = len(cols)\n",
    "    if cnt == 1:\n",
    "        col = cols[0]\n",
    "        fig, ax = plt.subplots(figsize=(5,3))\n",
    "        sns.boxplot(x=df[col], ax=ax);\n",
    "        #axes[i].set_title('Distribution')\n",
    "        ax.set_xlabel(col)\n",
    "    else:\n",
    "\n",
    "        fig, axes = plt.subplots(cnt, 1, figsize=(5,3*cnt))\n",
    "\n",
    "        for i, col in enumerate(cols):\n",
    "            sns.boxplot(x=df[col], ax=axes[i]);\n",
    "            #axes[i].set_title('Distribution')\n",
    "            axes[i].set_xlabel(col)\n",
    "            #axes[i].set_ylabel('Count');\n",
    "        fig.tight_layout()\n",
    "    return\n",
    "\n",
    "\n",
    "def makeHeatmap(df, cols, annot = False, line=False, width=12, height=10):\n",
    "    \"\"\"\n",
    "    This function will calculate and show heatmap of correlation\n",
    "    Input: df: a dataframe, \n",
    "           cols: a list of numerical features.\n",
    "           annot: a conditional flag. If True, then the value of correlation will be printed within heatmap.\n",
    "           line = a conditional flag. If True, then cell border line will be printed inside heatmap.\n",
    "    output: This function only shows heatmap.\n",
    "    \"\"\"\n",
    "    #colormap = plt.cm.white\n",
    "    colormap = plt.cm.YlGnBu\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(width,height))\n",
    "    plt.title('Correlation Heatmap', size=15)\n",
    "    if(line == True):\n",
    "        sns.heatmap(df[cols].corr(), vmax=1.0, cmap=colormap, annot=annot, square=True, linewidths=0.005, linecolor='gray');\n",
    "    elif(line == False):\n",
    "        sns.heatmap(df[cols].corr(), vmax=1.0, cmap=colormap, annot=annot, square=True);\n",
    "    return\n",
    "\n",
    "def make_categorydf(df, metadata, catName):\n",
    "    cols = metadata[metadata['category']==catName].index.tolist()\n",
    "    res_df = pd.DataFrame(columns={'feature', 'uniqueCnt'})\n",
    "    res_df['feature'] = cols\n",
    "\n",
    "    for col in cols:\n",
    "        res_df.loc[res_df['feature']==col,'uniqueCnt'] = df[col].nunique()\n",
    "\n",
    "    # To set order of columns in dataframe:\n",
    "    res_df = res_df[['feature', 'uniqueCnt']]\n",
    "    return res_df\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "def make_typedf(df, metadata, typeName):\n",
    "    cols = metadata[metadata['type']==typeName].index.tolist()\n",
    "    \n",
    "    if 'id' in cols:\n",
    "        cols.remove('id')\n",
    "    \n",
    "    \n",
    "    cnt = len(cols)\n",
    "    \n",
    "    res_df = pd.DataFrame(columns={'feature', 'uniqueCnt'})\n",
    "    res_df['feature'] = cols\n",
    "\n",
    "    for col in cols:\n",
    "        res_df.loc[res_df['feature']==col,'uniqueCnt'] = df[col].nunique()\n",
    "\n",
    "    # To set order of columns in dataframe:\n",
    "    res_df = res_df[['feature', 'uniqueCnt']]\n",
    "    \n",
    "    \n",
    "    return cols, cnt, res_df\n",
    "    \n",
    "def checkMissValue(df, metadata):\n",
    "\n",
    "    res = pd.DataFrame(index = df.columns, columns = ['count', 'percentage', 'type'])\n",
    "\n",
    "    for col in df.columns:\n",
    "        temp = df[col]==-1\n",
    "        try:\n",
    "            res.loc[col, 'count'] = int(temp.value_counts()[1])\n",
    "        except:\n",
    "            res.loc[col, 'count'] = 0\n",
    "\n",
    "    res['percentage'] = (res['count']/df.shape[0])*100\n",
    "    res.sort_values(by = 'count', inplace=True, ascending=False)\n",
    "\n",
    "    # we can also save the type of feature:\n",
    "    for x in res.index:\n",
    "        res.loc[x, 'type'] = metadata.loc[x, 'type']\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def levelFrequency(df, cols):\n",
    "    \"\"\"\n",
    "    This function checks that whether we have features that their values are are dominant only by one value or not?\n",
    "    \"\"\"\n",
    "    frequencyLst = []\n",
    "    for col in cols:\n",
    "        dominant = df[col].value_counts().values[0]\n",
    "        dominant = np.round(dominant*100/df.shape[0], 2)\n",
    "        entry = {'feature':col, 'frequency':dominant}\n",
    "        frequencyLst.append(entry)\n",
    "    frequency_df = pd.DataFrame(frequencyLst, columns = ['feature', 'frequency'])\n",
    "    frequency_df.sort_values(by='frequency', ascending=False, inplace=True, ignore_index=True)\n",
    "    return frequency_df\n",
    "\n",
    "def identify_single_unique(df):\n",
    "    \"\"\"Finds features with only a single unique value. NaN do not count as a unique value. \"\"\"\n",
    "\n",
    "    # Calculate the unique counts in each column\n",
    "    unique_counts = df.nunique()\n",
    "    unique_table = pd.DataFrame(unique_counts).reset_index().rename(columns = {'index': 'feature', 0: 'levels'})\n",
    "    unique_table.sort_values('levels', inplace = True, ascending = True)\n",
    "\n",
    "    # Find the columns with only one unique count\n",
    "    single_count = unique_table[unique_table['levels']==1] \n",
    "   \n",
    "    print('There are {:d} features with a single unique value.'.format(len(single_count)))\n",
    "    return list(single_count['feature'])\n",
    "\n",
    "\n",
    "def change_datatype(df): #minimize used memory\n",
    "    for col in list(df.select_dtypes(include=['int']).columns):\n",
    "        if df[col].max() < 2**7 and df[col].min() >= -2**7:\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "        elif df[col].max() < 2**8 and df[col].min() >= 0:\n",
    "            df[col] = df[col].astype(np.uint8)\n",
    "        elif df[col].max() < 2**15 and df[col].min() >= -2**15:\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "        elif df[col].max() < 2**16 and df[col].min() >= 0:\n",
    "            df[col] = df[col].astype(np.uint16)\n",
    "        elif df[col].max() < 2**31 and df[col].min() >= -2**31:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "        elif df[col].max() < 2**32 and df[col].min() >= 0:\n",
    "            df[col] = df[col].astype(np.uint32)\n",
    "    for col in list(df.select_dtypes(include=['float']).columns):\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "\n",
    "def size_MB(obj):\n",
    "    import sys\n",
    "    size = sys.getsizeof(obj)\n",
    "    size /=10**6\n",
    "    size = round(size, 2)\n",
    "    return size\n",
    "\n",
    "\n",
    "def imputby_Regression(data_df, corr_df, useOneCorrFeature=True):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    LR = LinearRegression()\n",
    "    totalCnt = data_df.shape[0]\n",
    "\n",
    "    for i in range(corr_df.shape[0]):\n",
    "        refVar = corr_df.loc[i, 'miss_feat']\n",
    "\n",
    "        if (useOneCorrFeature == True):\n",
    "            if(corr_df.loc[i, 'weight_1'] >= corr_df.loc[i, 'weight_2']):\n",
    "                corVar = corr_df.loc[i, 'corr_feat1']\n",
    "                missCnt = corr_df.loc[i, 'miss_count1']\n",
    "            else:\n",
    "                corVar = corr_df.loc[i, 'corr_feat2']\n",
    "                missCnt = corr_df.loc[i, 'miss_count2']\n",
    "            \n",
    "        elif(useOneCorrFeature == False):\n",
    "            missCnt1 = corr_df.loc[i, 'miss_count1']\n",
    "            missCnt2 = corr_df.loc[i, 'miss_count2']\n",
    "            corVar1 = corr_df.loc[i, 'corr_feat1']\n",
    "            corVar2 = corr_df.loc[i, 'corr_feat2']\n",
    "\n",
    "        \n",
    "        # I have found the regression works for numerical reference and categorical correlated,\n",
    "        # Because, categorical features are initially encoded by digits in original dataset.\n",
    "        # We dont care about the real concept of the value of the categorical, but it has a correlation with refVar.\n",
    "        # So if the reference is numerical, I can use regression.\n",
    "\n",
    "        nullindexRef = data_df[refVar].isnull().values\n",
    "        \n",
    "        if (useOneCorrFeature == True):\n",
    "            if(missCnt==0):\n",
    "                nullindex = nullindexRef\n",
    "            else:\n",
    "                nullindexCorr = data_df[corVar].isnull().values\n",
    "                nullindex = [True if (nullindexRef[i] == True and nullindexCorr[i] == True) else False for i in range(totalCnt)]\n",
    "        else:\n",
    "            if(missCnt1==0 & missCnt2==0):\n",
    "                nullindex = nullindexRef\n",
    "            else:\n",
    "                nullindexCorr1 = data_df[corVar1].isnull().values\n",
    "                nullindexCorr2 = data_df[corVar2].isnull().values\n",
    "                nullindex = [True if (nullindexRef[i] == True and nullindexCorr1[i] == True and nullindexCorr2[i] == True)\n",
    "                         else False for i in range(totalCnt)]\n",
    "\n",
    "        # For training Regression model, We need index of not-missing rows.\n",
    "        notnullindex = ~nullindex\n",
    "\n",
    "\n",
    "        # X1: is defined as the first correlated feature.\n",
    "        # X2: is defined as the second correlated feature.\n",
    "        # y: is defined the feature with miss data.\n",
    "        \n",
    "        if (useOneCorrFeature == True):\n",
    "            XforTrain = data_df.loc[notnullindex, corVar1].values.reshape(-1,1)\n",
    "            XforPred = data_df.loc[nullindex, corVar1].values.reshape(-1,1)\n",
    "        \n",
    "        if (useOneCorrFeature == False):\n",
    "            X1forTrain = data_df.loc[notnullindex, corVar1].values\n",
    "            X2forTrain = data_df.loc[notnullindex, corVar2].values\n",
    "            X1forPred = data_df.loc[nullindex, corVar1].values\n",
    "            X2forPred = data_df.loc[nullindex, corVar2].values\n",
    "            \n",
    "            \n",
    "            XforTrain = pd.DataFrame({'x1':X1forTrain, 'x2':X2forTrain, 'x1x2': X1forTrain*X2forTrain})\n",
    "            XforPred = pd.DataFrame({'x1':X1forPred, 'x2':X2forPred, 'x1x2': X1forPred*X2forPred})\n",
    "        \n",
    "        y = data_df.loc[notnullindex, refVar].values.reshape(-1,1)\n",
    "        LR.fit(XforTrain, y)        \n",
    "        # After training, we will fill the miss data:\n",
    "        data_df.loc[nullindex, refVar] = LR.predict(XforPred)\n",
    "\n",
    "  \n",
    "        print('After imputing, Rows with missed data: {:d}'.format(data_df[refVar].isnull().sum()))\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "\n",
    "def make_correlated_table(df, colArray):\n",
    "    \"\"\"\n",
    "    This function will make a correlation matrix, for imputing purposes of numerical features:\n",
    "    input: df:a dataframe\n",
    "           colArray: a list of numerical featur name \n",
    "    \"\"\"\n",
    "\n",
    "    correlated_feat1 = []\n",
    "    correlated_feat2 = []\n",
    "\n",
    "    correlated_val1 = []\n",
    "    correlated_val2 = []\n",
    "\n",
    "    for col in colArray:\n",
    "        corr = df.corr()[col]\n",
    "        corr = abs(corr).sort_values(ascending = False)\n",
    "\n",
    "        correlated_feat1.append(corr.index[1])\n",
    "        correlated_feat2.append(corr.index[2])\n",
    "\n",
    "        correlated_val1.append(corr[1])\n",
    "        correlated_val2.append(corr[2])\n",
    "\n",
    "\n",
    "    time1 = time.process_time()\n",
    "    colLst = ['miss_feat',\n",
    "              'corr_feat1', 'corr_val1', 'miss_count1', 'weight_1',\n",
    "              'corr_feat2', 'corr_val2', 'miss_count2', 'weight_2']\n",
    "    row_list = []\n",
    "    totalLen = df.shape[0]\n",
    "    for i, col in enumerate(colArray):\n",
    "\n",
    "        feat1 = correlated_feat1[i]\n",
    "        val1 = correlated_val1[i]\n",
    "        miss1 = df[feat1].isnull().sum()\n",
    "        weight1 = val1 - miss1/totalLen\n",
    "\n",
    "        feat2 = correlated_feat2[i]\n",
    "        val2 = correlated_val2[i]\n",
    "        miss2 = df[feat2].isnull().sum()\n",
    "        weight2 = val2 - miss2/totalLen\n",
    "\n",
    "        valueLst = [col, feat1, val1, miss1, weight1, feat2, val2, miss2, weight2]\n",
    "        dic1 = {k:v for k,v in zip(colLst, valueLst)}\n",
    "        row_list.append(dic1)\n",
    "\n",
    "    corr_matrix = pd.DataFrame(row_list, columns = colLst)\n",
    "    time2 = time.process_time()\n",
    "    print('Elapsed time: {:6.3f} seconds'.format((time2- time1)))\n",
    "\n",
    "    return corr_matrix\n",
    "\n",
    "\n",
    "def findVariance(df):\n",
    "   \n",
    "    cols = df.columns.tolist()\n",
    "    res_df = pd.DataFrame(columns = {'feature', 'variance'})\n",
    "    res_df['feature'] = cols\n",
    "    res_df['variance'] = [np.var(df[col]) for col in cols]\n",
    "\n",
    "    # We set the order of columns\n",
    "    colOrder = ['feature', 'variance']\n",
    "    res_df = res_df[colOrder]\n",
    "\n",
    "    #Sorting based on the absolute value of correlation\n",
    "    res_df.sort_values(by='variance', inplace=True)\n",
    "    return res_df  \n",
    "\n",
    "\n",
    "def feature_selection_RandomForset(X_input, y_input, threshold_imp = 0.1, threshold_cumulative = 0.98, cv_input=5, alphas_input=[0.02, 0.05, 0.08]):\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "   \n",
    "    if(y_input.iloc[:,0].dtypes == 'object'):\n",
    "        rf = RandomForestRegressor(random_state = 0, n_jobs = -1)\n",
    "    else:\n",
    "        rf = RandomForestClassifier(random_state = 0, n_jobs = -1)\n",
    "        \n",
    "    # Fitting the classifier\n",
    "    rf.fit(X_input, y_input)\n",
    "\n",
    "    # Printing the name of each feature along with the gini value\n",
    "    imp_values = list(rf.feature_importances_)\n",
    "    imp_table = pd.DataFrame({'feature':list(X_input.columns), 'importance':imp_values})\n",
    "    \n",
    " \n",
    "    # Finally selecting the most important features\n",
    "    sfm = SelectFromModel(rf, threshold=0.15)\n",
    "    sfm.fit(X_input, y_input)\n",
    "   \n",
    "\n",
    "    # Extracting the index of important features\n",
    "    index_selected = list(sfm.get_support())\n",
    "    feature_selected = X_input.columns[index_selected]\n",
    "    total = X_input.shape[1]\n",
    "    selected = len(feature_selected)\n",
    "    removed = total - selected\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    imp_table.sort_values('importance', ascending = False, inplace = True)\n",
    "    imp_table.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Normalizint the feature importances:\n",
    "    imp_table['normalized_importance'] = imp_table['importance'] / imp_table['importance'].sum()\n",
    "    imp_table['cumulative_importance'] = np.cumsum(imp_table['normalized_importance'])\n",
    "\n",
    "    # Extract the features with importance greter than threshold\n",
    "    zero_imp = imp_table.loc[imp_table['importance'] == 0.0, ['feature', 'importance']]\n",
    "    low_imp = imp_table[imp_table['importance'] < threshold_imp]\n",
    "    high_imp = imp_table[imp_table['importance'] >= threshold_imp]    \n",
    "    \n",
    "    cumulative_imp = pd.DataFrame({'count': [i for i in range(1, X_input.shape[1]+1)]})\n",
    "    cumulative_imp['cumulative_importance'] = imp_table['cumulative_importance']\n",
    "    required_feature = 0\n",
    "    for i in range(cumulative_imp.shape[0]):\n",
    "        if cumulative_imp.iloc[i, 1]>= threshold_cumulative:\n",
    "            required_feature = cumulative_imp.iloc[i, 0]\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print('\\nThe original data has {:d} features.'.format(total))    \n",
    "    print('After one-hot encoding, number of features becoms {:d}'.format(X.shape[1]))\n",
    "    print('{:d} features has zero importance.'.format(zero_imp.shape[0]))\n",
    "    print('{:d} features has lower importance than the given threshold.'.format(len(low_imp)))\n",
    "    print('There are {:d} features for cumulative threshold of {:.3f} importance.'.format(len(cumulative_imp), threshold_cumulative))\n",
    "    \"\"\"    \n",
    "    print('The original data has {:d} features. By applying importance threshold of {:0.3f}, we found:'.format(total, threshold_imp))    \n",
    "    print('{:d} features has been selected.'.format(selected))\n",
    "    print('{:d} features has been removed.'.format(removed))\n",
    "    \n",
    "    X_fs = sfm.transform(X_input)\n",
    "    print('\\nBefore feature selection: {}'.format(X_input.shape))\n",
    "    print('After feature selection: {}'.format(X_fs.shape))\n",
    "    \n",
    "    #return index_selected\n",
    "    return imp_table, low_imp, zero_imp, cumulative_imp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
